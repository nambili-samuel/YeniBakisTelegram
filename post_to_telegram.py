#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import feedparser
import requests
from io import BytesIO
from PIL import Image
import re
from datetime import datetime
from bs4 import BeautifulSoup
import json
import time

# Force UTF-8 encoding for stdout
if sys.stdout.encoding != 'utf-8':
    sys.stdout.reconfigure(encoding='utf-8')

RSS_URL = os.environ.get("RSS_URL", "https://www.yenibakishaber.com/rss")
TELEGRAM_BOT_TOKEN = os.environ["TELEGRAM_BOT_TOKEN"]
TELEGRAM_CHAT_ID = os.environ["TELEGRAM_CHAT_ID"]

# Track ALL posted links, not just the last one
STATE_FILE = "posted_links.json"
MAX_IMAGE_SIZE = 10_000_000  # 10MB (Telegram limit)
# How many RSS entries to check each run (to avoid spamming)
MAX_ENTRIES_TO_PROCESS = 10
# Minimum time between posts to avoid rate limits (seconds)
POST_DELAY = 2

TELEGRAM_API_URL = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}"

def load_posted_links():
    """Load all previously posted article links"""
    if os.path.exists(STATE_FILE):
        try:
            with open(STATE_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                # Backward compatibility: if it's a string (old format), convert to dict
                if isinstance(data, str):
                    return {data: datetime.now().isoformat()}
                return data
        except (json.JSONDecodeError, IOError):
            return {}
    return {}

def save_posted_links(links_dict):
    """Save all posted article links with timestamps"""
    # Keep only the last 1000 entries to prevent file from growing too large
    if len(links_dict) > 1000:
        # Sort by timestamp and keep most recent
        sorted_items = sorted(links_dict.items(), key=lambda x: x[1], reverse=True)
        links_dict = dict(sorted_items[:1000])
    
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(links_dict, f, ensure_ascii=False, indent=2)
    print(f"âœ… {len(links_dict)} link kaydedildi")

def mark_as_posted(link):
    """Mark a link as posted with current timestamp"""
    posted_links = load_posted_links()
    posted_links[link] = datetime.now().isoformat()
    save_posted_links(posted_links)
    print(f"âœ… Link iÅŸaretlendi: {link[:50]}...")

def optimize_image(image_data):
    """Optimize image to fit within size limit while maintaining quality"""
    try:
        img = Image.open(BytesIO(image_data))
        
        # Convert to RGB if necessary
        if img.mode in ('RGBA', 'P', 'LA'):
            background = Image.new('RGB', img.size, (255, 255, 255))
            if img.mode == 'P':
                img = img.convert('RGBA')
            if img.mode in ('RGBA', 'LA'):
                background.paste(img, mask=img.split()[-1])
            img = background
        
        # Resize if too large (maintain aspect ratio)
        max_dimension = 2000
        if max(img.size) > max_dimension:
            ratio = max_dimension / max(img.size)
            new_size = tuple(int(dim * ratio) for dim in img.size)
            img = img.resize(new_size, Image.Resampling.LANCZOS)
        
        # Save with progressive optimization
        output = BytesIO()
        quality = 85
        
        while quality > 20:
            output.seek(0)
            output.truncate()
            img.save(output, format='JPEG', quality=quality, optimize=True, progressive=True)
            
            if output.tell() <= MAX_IMAGE_SIZE:
                print(f"âœ… GÃ¶rsel optimize edildi: {len(image_data)} -> {output.tell()} bytes (kalite: {quality})")
                return output.getvalue()
            
            quality -= 5
        
        print("âš  GÃ¶rsel optimize edilemedi, boyut sÄ±nÄ±rÄ± aÅŸÄ±lÄ±yor")
        return None
        
    except Exception as e:
        print(f"âŒ GÃ¶rsel optimizasyon hatasÄ±: {e}")
        return None

def fetch_image(url):
    """Fetch and optimize image from URL"""
    if not url:
        return None
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7',
            'Referer': 'https://www.yenibakishaber.com/'
        }
        
        print(f"ğŸ“¥ GÃ¶rsel indiriliyor: {url}")
        r = requests.get(url, timeout=15, headers=headers, stream=True)
        r.raise_for_status()
        
        content = r.content
        print(f"âœ… GÃ¶rsel indirildi: {len(content)} bytes")
        
        # If image is already small enough, return it
        if len(content) <= MAX_IMAGE_SIZE:
            return content
        
        # Otherwise, optimize it
        return optimize_image(content)
        
    except Exception as e:
        print(f"âŒ GÃ¶rsel indirme hatasÄ± ({url}): {e}")
        return None

def clean_html(text):
    """Remove HTML tags and clean text"""
    if not text:
        return ""
    # Remove HTML tags
    text = re.sub(r'<[^>]+>', '', text)
    # Clean up whitespace
    text = re.sub(r'\s+', ' ', text)
    # Decode HTML entities
    text = text.replace('&nbsp;', ' ')
    text = text.replace('&#8217;', "'")
    text = text.replace('&#8220;', '"')
    text = text.replace('&#8221;', '"')
    text = text.replace('&#8216;', "'")
    text = text.replace('&quot;', '"')
    text = text.replace('&apos;', "'")
    text = text.replace('&amp;', '&')
    text = text.replace('&lt;', '<')
    text = text.replace('&gt;', '>')
    # Remove CDATA markers
    text = text.replace('<![CDATA[', '')
    text = text.replace(']]>', '')
    return text.strip()

def extract_intro_from_content(summary, max_length=200):
    """Extract a clean introduction from the summary/description"""
    if not summary:
        return None
    
    # Clean the summary
    intro = clean_html(summary)
    
    # Remove common patterns
    intro = re.sub(r'(DevamÄ± iÃ§in|Haberi oku|TÄ±klayÄ±n|Kaynak:).*$', '', intro, flags=re.IGNORECASE)
    intro = re.sub(r'https?://\S+', '', intro)  # Remove URLs
    
    # Trim to reasonable length at sentence boundary
    if len(intro) > max_length:
        # Try to cut at sentence end
        sentences = re.split(r'[.!?]+\s+', intro)
        intro = sentences[0]
        if len(intro) > max_length:
            # If still too long, cut at word boundary
            intro = intro[:max_length].rsplit(' ', 1)[0] + '...'
        else:
            intro += '.'
    
    intro = intro.strip()
    
    # Return only if we have meaningful content
    if len(intro) > 30:
        return intro
    
    return None

def fetch_article_content(article_url):
    """Fetch article content including intro/description from the page"""
    if not article_url or article_url == '#':
        return None
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7',
        }
        
        print(f"ğŸ“„ Makale iÃ§eriÄŸi Ã§ekiliyor: {article_url}")
        response = requests.get(article_url, timeout=15, headers=headers)
        response.encoding = 'utf-8'
        
        if response.status_code != 200:
            print(f"âš  Makale sayfasÄ± yÃ¼klenemedi: HTTP {response.status_code}")
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Try Open Graph description first (most reliable)
        og_desc = soup.find('meta', property='og:description')
        if og_desc and og_desc.get('content'):
            content = og_desc['content'].strip()
            if len(content) > 30:
                print(f"âœ… Ä°Ã§erik bulundu (og:description): {len(content)} karakter")
                return content
        
        # Try meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            content = meta_desc['content'].strip()
            if len(content) > 30:
                print(f"âœ… Ä°Ã§erik bulundu (meta description): {len(content)} karakter")
                return content
        
        # Try first paragraph in article content
        content_selectors = [
            '.entry-content p',
            'article p',
            '.post-content p',
            '.content p',
            'main p'
        ]
        
        for selector in content_selectors:
            paragraphs = soup.select(selector)
            for p in paragraphs[:3]:  # Check first 3 paragraphs
                text = p.get_text().strip()
                if len(text) > 50:  # Meaningful paragraph
                    print(f"âœ… Ä°Ã§erik bulundu (article paragraph): {len(text)} karakter")
                    return text
        
        print("âš  Makale iÃ§eriÄŸi bulunamadÄ±")
        return None
        
    except Exception as e:
        print(f"âŒ Makale iÃ§eriÄŸi okuma hatasÄ±: {e}")
        return None

def fetch_article_thumbnail(article_url):
    """Fetch featured image from WordPress article page"""
    if not article_url or article_url == '#':
        return None
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7',
        }
        
        print(f"ğŸŒ Makale sayfasÄ± aÃ§Ä±lÄ±yor: {article_url}")
        response = requests.get(article_url, timeout=15, headers=headers)
        response.encoding = 'utf-8'
        
        if response.status_code != 200:
            print(f"âš  Makale sayfasÄ± yÃ¼klenemedi: HTTP {response.status_code}")
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Method 1: Open Graph image (most reliable for WordPress)
        og_image = soup.find('meta', property='og:image')
        if og_image and og_image.get('content'):
            url = og_image['content']
            print(f"âœ… Thumbnail bulundu (og:image): {url}")
            return url
        
        # Method 2: Twitter card image
        twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})
        if twitter_image and twitter_image.get('content'):
            url = twitter_image['content']
            print(f"âœ… Thumbnail bulundu (twitter:image): {url}")
            return url
        
        # Method 3: WordPress featured image
        featured_img = soup.select_one('.wp-post-image, .featured-image img, article img, .entry-content img')
        if featured_img and featured_img.get('src'):
            url = featured_img['src']
            # Skip tiny images
            if 'placeholder' not in url.lower() and '1x1' not in url.lower():
                print(f"âœ… Thumbnail bulundu (featured image): {url}")
                return url
        
        # Method 4: First large image in content
        content_images = soup.select('article img, .entry-content img, .post-content img')
        for img in content_images:
            src = img.get('src') or img.get('data-src')
            if src and 'placeholder' not in src.lower() and '1x1' not in src.lower():
                # Check if image has reasonable dimensions
                width = img.get('width', '0')
                height = img.get('height', '0')
                try:
                    if int(width) >= 300 or int(height) >= 300:
                        print(f"âœ… Thumbnail bulundu (content image): {src}")
                        return src
                except:
                    print(f"âœ… Thumbnail bulundu (content image): {src}")
                    return src
        
        print("âš  Makale sayfasÄ±nda thumbnail bulunamadÄ±")
        return None
        
    except Exception as e:
        print(f"âŒ Makale sayfasÄ± okuma hatasÄ±: {e}")
        return None

def extract_youtube_thumbnail(entry, link):
    """Extract high-quality thumbnail from YouTube"""
    video_id = None
    
    # Try to get video ID from entry
    if hasattr(entry, 'yt_videoid'):
        video_id = entry.yt_videoid
    elif hasattr(entry, 'id'):
        # YouTube RSS sometimes has video ID in the id field
        id_str = str(entry.id)
        if 'yt:video:' in id_str:
            video_id = id_str.split('yt:video:')[-1]
    
    # Extract from link if not found
    if not video_id and 'v=' in link:
        video_id = link.split('v=')[1].split('&')[0]
    
    if video_id:
        print(f"âœ… YouTube video ID: {video_id}")
        
        # Try different quality thumbnails in order of preference
        thumbnail_urls = [
            f"https://i.ytimg.com/vi/{video_id}/maxresdefault.jpg",
            f"https://i.ytimg.com/vi/{video_id}/sddefault.jpg",
            f"https://i.ytimg.com/vi/{video_id}/hqdefault.jpg",
        ]
        
        for url in thumbnail_urls:
            try:
                headers = {'User-Agent': 'Mozilla/5.0'}
                r = requests.head(url, timeout=5, headers=headers, allow_redirects=True)
                if r.status_code == 200:
                    print(f"âœ… YouTube thumbnail bulundu: {url}")
                    return url
            except:
                continue
    
    # Fallback to media_thumbnail
    if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
        url = entry.media_thumbnail[0].get('url')
        if url:
            print(f"âœ… YouTube media_thumbnail kullanÄ±lÄ±yor: {url}")
            return url
    
    print("âš  YouTube thumbnail bulunamadÄ±")
    return None

def create_beautiful_post(title, link, category="", intro=None):
    """Create a beautiful, professional Telegram post with heading and introduction"""
    
    # Decode HTML entities in title
    title = clean_html(title)
    
    # Add appropriate emoji based on category
    category_emojis = {
        'SPOR': 'âš½',
        'EKONOMÄ°': 'ğŸ’°',
        'DÃœNYA': 'ğŸŒ',
        'TÃœRKÄ°YE': 'ğŸ‡¹ğŸ‡·',
        'KIBRIS': 'ğŸ‡¨ğŸ‡¾',
        'GÃœNEY KIBRIS': 'ğŸ‡¨ğŸ‡¾',
        'SAÄLIK': 'ğŸ¥',
        'TEKNOLOJÄ°': 'ğŸ’»',
        'KÃœLTÃœR': 'ğŸ­',
        'SANAT': 'ğŸ¨',
        'GENEL': 'ğŸ“°',
        'POLÄ°TÄ°KA': 'ğŸ›ï¸',
        'HABER': 'ğŸ“¢',
        'GÃœNCEL': 'ğŸ†•',
        'Ä°ZMÄ°R': 'ğŸ™ï¸',
        'AYDIN': 'ğŸŒ†',
        'MAGAZÄ°N': 'â­',
        'HAYAT': 'ğŸŒŸ',
        'EÄÄ°TÄ°M': 'ğŸ“š',
        'BÄ°LÄ°M': 'ğŸ”¬',
        'Ã‡EVRE': 'ğŸŒ±',
        'OTOMOTÄ°V': 'ğŸš—',
    }
    
    # Try to find matching category
    emoji = 'ğŸ“°'  # Default
    for cat_key, cat_emoji in category_emojis.items():
        if cat_key in category.upper():
            emoji = cat_emoji
            break
    
    # Create post with enhanced professional formatting
    # Header with decorative line
    post_text = f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n"
    post_text += f"  {emoji} <b>{category.upper()}</b>\n"
    post_text += f"â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›\n\n"
    
    # Main title - bold and prominent
    post_text += f"<b>{title}</b>\n\n"
    
    # Introduction if available
    if intro:
        intro_text = extract_intro_from_content(intro, max_length=180)
        if intro_text:
            post_text += f"ğŸ“ <i>{intro_text}</i>\n\n"
    
    # Divider
    post_text += f"{'â”€' * 30}\n\n"
    
    # Call to action with link
    post_text += f"ğŸ“– <a href='{link}'>Haberin TamamÄ±nÄ± Oku</a>\n\n"
    
    # Timestamp
    current_time = datetime.now().strftime('%d.%m.%Y â€¢ %H:%M')
    post_text += f"ğŸ• <i>{current_time}</i>"
    
    return post_text

def send_telegram_message(text, image_data=None):
    """Send message to Telegram with optional image"""
    try:
        if image_data:
            # Send photo with caption
            files = {
                'photo': ('image.jpg', BytesIO(image_data), 'image/jpeg')
            }
            data = {
                'chat_id': TELEGRAM_CHAT_ID,
                'caption': text,
                'parse_mode': 'HTML'
            }
            response = requests.post(
                f"{TELEGRAM_API_URL}/sendPhoto",
                data=data,
                files=files,
                timeout=30
            )
        else:
            # Send text only
            data = {
                'chat_id': TELEGRAM_CHAT_ID,
                'text': text,
                'parse_mode': 'HTML',
                'disable_web_page_preview': False
            }
            response = requests.post(
                f"{TELEGRAM_API_URL}/sendMessage",
                data=data,
                timeout=30
            )
        
        response.raise_for_status()
        result = response.json()
        
        if result.get('ok'):
            return True
        else:
            print(f"âŒ Telegram API hatasÄ±: {result}")
            return False
            
    except Exception as e:
        print(f"âŒ Telegram gÃ¶nderim hatasÄ±: {e}")
        return False

def post_to_telegram(entry):
    """Post a single entry to Telegram"""
    title = clean_html(entry.title)
    link = entry.link
    summary = clean_html(entry.get("summary", entry.get("description", "")))
    
    # Extract category
    categories = []
    if hasattr(entry, 'tags'):
        categories = [tag.term for tag in entry.tags]
    category = categories[0] if categories else "GENEL"
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Œ Ä°ÅŸleniyor: {title[:80]}...")
    print(f"   Kategori: {category}")
    print(f"   Link: {link}")
    print(f"{'='*60}")
    
    # Detect source type and extract thumbnail
    is_youtube = 'youtube.com' in RSS_URL.lower() or 'youtu.be' in RSS_URL.lower()
    
    thumbnail_url = None
    intro_text = None
    
    if is_youtube:
        thumbnail_url = extract_youtube_thumbnail(entry, link)
        # For YouTube, use the summary as intro
        intro_text = summary
    else:
        # For WordPress, try RSS enclosure first, then fetch from article page
        if hasattr(entry, 'enclosures') and entry.enclosures:
            enclosure_url = entry.enclosures[0].get('url') or entry.enclosures[0].get('href')
            if enclosure_url:
                print(f"âœ… RSS enclosure bulundu: {enclosure_url}")
                thumbnail_url = enclosure_url
        
        # Try to get intro from RSS summary first
        if summary and len(summary) > 30:
            intro_text = summary
            print(f"âœ… RSS Ã¶zeti kullanÄ±lÄ±yor: {len(summary)} karakter")
        
        # If no thumbnail or intro, fetch from article page
        if not thumbnail_url or not intro_text:
            # Fetch article page content once for both thumbnail and intro
            try:
                article_content = fetch_article_content(link)
                if article_content and not intro_text:
                    intro_text = article_content
                    
                if not thumbnail_url:
                    thumbnail_url = fetch_article_thumbnail(link)
            except Exception as e:
                print(f"âš  Makale sayfasÄ± iÅŸlenirken hata: {e}")
    
    # Fetch thumbnail image
    image_data = None
    if thumbnail_url:
        print(f"ğŸ”¸ Thumbnail iÅŸleniyor...")
        image_data = fetch_image(thumbnail_url)
        
        if image_data:
            print(f"âœ… Thumbnail hazÄ±r")
        else:
            print(f"âš  Thumbnail indirilemedi, devam ediliyor...")
    else:
        print(f"âš  Thumbnail bulunamadÄ±, devam ediliyor...")
    
    # Create beautiful post text with intro
    post_text = create_beautiful_post(title, link, category, intro_text)
    
    # Post to Telegram
    print(f"\nğŸ“¤ Telegram'a gÃ¶nderiliyor...")
    
    success = send_telegram_message(post_text, image_data)
    
    if success:
        mark_as_posted(link)
        
        print(f"\nâœ… BAÅARIYLA PAYLAÅILDI!")
        print(f"ğŸ“Œ BaÅŸlÄ±k: {title}")
        print(f"ğŸ“‚ Kategori: {category}")
        print(f"ğŸ“ Ä°Ã§erik: {'Var âœ”' if intro_text else 'Yok âœ—'}")
        print(f"ğŸ”— Link: {link}")
        print(f"ğŸ–¼ï¸ Thumbnail: {'Evet âœ”' if image_data else 'HayÄ±r âœ—'}")
        print(f"â° Zaman: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        return True
    else:
        print(f"\nâŒ PAYLAÅIM HATASI!")
        return False

# Parse RSS feed with UTF-8 support
print(f"\n{'='*60}")
print(f"ğŸ“° RSS Feed Ä°ÅŸleniyor: {RSS_URL}")
print(f"{'='*60}\n")

feed = feedparser.parse(RSS_URL)

if not feed.entries:
    print("âš  RSS beslemesinde iÃ§erik bulunamadÄ±.")
    sys.exit(0)

print(f"âœ… RSS beslemesi okundu: {len(feed.entries)} iÃ§erik bulundu\n")

# Load previously posted links
posted_links = load_posted_links()
print(f"ğŸ“Š Daha Ã¶nce paylaÅŸÄ±lan link sayÄ±sÄ±: {len(posted_links)}")

# Test Telegram connection
print(f"\nğŸ” Telegram baÄŸlantÄ±sÄ± test ediliyor...")
try:
    response = requests.get(f"{TELEGRAM_API_URL}/getMe", timeout=10)
    response.raise_for_status()
    bot_info = response.json()
    if bot_info.get('ok'):
        bot_name = bot_info['result']['first_name']
        print(f"âœ… Telegram baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±: {bot_name}\n")
    else:
        print(f"âŒ Telegram baÄŸlantÄ± hatasÄ±: {bot_info}")
        sys.exit(1)
except Exception as e:
    print(f"âŒ Telegram baÄŸlantÄ± hatasÄ±: {e}")
    sys.exit(1)

# Process entries in reverse order (oldest to newest) to maintain chronological order
# But limit to most recent entries to avoid processing too many
entries_to_process = feed.entries[:MAX_ENTRIES_TO_PROCESS]
print(f"â³ Ä°ÅŸlenecek iÃ§erik sayÄ±sÄ±: {len(entries_to_process)}")

new_posts_count = 0

# Process from newest to oldest (so newest appears first)
for i, entry in enumerate(entries_to_process):
    link = entry.link
    
    # Skip if already posted
    if link in posted_links:
        title = clean_html(entry.title)[:60]
        print(f"\nâ­• Zaten paylaÅŸÄ±ldÄ±: {title}...")
        continue
    
    # Post to Telegram
    success = post_to_telegram(entry)
    if success:
        new_posts_count += 1
        
        # Add delay between posts to avoid rate limits (except for last post)
        if i < len(entries_to_process) - 1:
            print(f"\nâ³ {POST_DELAY} saniye bekleniyor...")
            time.sleep(POST_DELAY)

print(f"\n{'='*60}")
print(f"ğŸ“Š Ä°ÅLEM TAMAMLANDI")
print(f"{'='*60}")
print(f"Toplam kontrol edilen iÃ§erik: {len(entries_to_process)}")
print(f"Yeni paylaÅŸÄ±lan iÃ§erik: {new_posts_count}")
print(f"Zaten paylaÅŸÄ±lmÄ±ÅŸ iÃ§erik: {len(entries_to_process) - new_posts_count}")
print(f"Toplam kayÄ±tlÄ± link: {len(load_posted_links())}")
print(f"{'='*60}\n")

if new_posts_count == 0:
    print("â„¹ï¸ Yeni iÃ§erik bulunamadÄ±.")
